# MPL Explain

## 1. Gi·ªõi thi·ªáu

üìå Notebook n√†y s·ª≠ d·ª•ng **M·∫°ng n∆°-ron MLP (Multi-Layer Perceptron)** ƒë·ªÉ hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh d·ª± ƒëo√°n.

---

## 2. Import th∆∞ vi·ªán

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

**Gi·∫£i th√≠ch:**

- `numpy` & `pandas`: H·ªó tr·ª£ x·ª≠ l√Ω d·ªØ li·ªáu.
- `matplotlib.pyplot`: Tr·ª±c quan h√≥a d·ªØ li·ªáu.
- `tensorflow.keras`: X√¢y d·ª±ng m√¥ h√¨nh MLP.
- `Sequential`: T·∫°o m√¥ h√¨nh theo t·ª´ng l·ªõp.
- `Dense`: Layer fully connected c·ªßa m·∫°ng n∆°-ron.
- `train_test_split`: Chia d·ªØ li·ªáu th√†nh t·∫≠p train/test.
- `StandardScaler`: Chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ kho·∫£ng c√≥ gi√° tr·ªã trung b√¨nh 0 v√† ph∆∞∆°ng sai 1.

---

## 3. ƒê·ªçc d·ªØ li·ªáu

```python
df = pd.read_csv("data.csv")
print(df.head())
```

**Gi·∫£i th√≠ch:**

- ƒê·ªçc d·ªØ li·ªáu t·ª´ file `data.csv`.
- Hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra d·ªØ li·ªáu.

---

## 4. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu

```python
df = df.dropna()
X = df.drop(columns=["target"])
y = df["target"]
```

**Gi·∫£i th√≠ch:**

- `dropna()`: Lo·∫°i b·ªè c√°c d√≤ng ch·ª©a gi√° tr·ªã khuy·∫øt thi·∫øu.
- `X`: Ch·ª©a c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o.
- `y`: Nh√£n c·∫ßn d·ª± ƒëo√°n.

---

## 5. Chia t·∫≠p d·ªØ li·ªáu

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

**Gi·∫£i th√≠ch:**

- `test_size=0.2`: 20% d·ªØ li·ªáu d√†nh cho ki·ªÉm tra, 80% ƒë·ªÉ hu·∫•n luy·ªán.
- `random_state=42`: ƒê·∫£m b·∫£o k·∫øt qu·∫£ c√≥ th·ªÉ t√°i l·∫≠p.

---

## 6. Chu·∫©n h√≥a d·ªØ li·ªáu

```python
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

**Gi·∫£i th√≠ch:**

- `StandardScaler()`: Chu·∫©n h√≥a d·ªØ li·ªáu gi√∫p m√¥ h√¨nh h·ªçc hi·ªáu qu·∫£ h∆°n.
- `fit_transform(X_train)`: √Åp d·ª•ng chu·∫©n h√≥a tr√™n t·∫≠p train.
- `transform(X_test)`: √Åp d·ª•ng chu·∫©n h√≥a tr√™n t·∫≠p test b·∫±ng c√πng scaler.

---

## 7. X√¢y d·ª±ng m√¥ h√¨nh MLP

```python
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

**Gi·∫£i th√≠ch:**

- `Sequential()`: Kh·ªüi t·∫°o m√¥ h√¨nh theo t·ª´ng l·ªõp.
- `Dense(64, activation='relu')`: L·ªõp ·∫©n ƒë·∫ßu ti√™n c√≥ 64 neuron, d√πng ReLU.
- `Dense(32, activation='relu')`: L·ªõp ·∫©n th·ª© hai c√≥ 32 neuron.
- `Dense(1, activation='sigmoid')`: L·ªõp ƒë·∫ßu ra, d√πng Sigmoid v√¨ b√†i to√°n nh·ªã ph√¢n.
- `compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])`: C·∫•u h√¨nh t·ªëi ∆∞u h√≥a v√† h√†m m·∫•t m√°t.

---

## 8. Hu·∫•n luy·ªán m√¥ h√¨nh

```python
history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))
```

**Gi·∫£i th√≠ch:**

- `epochs=20`: Hu·∫•n luy·ªán m√¥ h√¨nh trong 20 v√≤ng l·∫∑p.
- `batch_size=16`: M·ªói l·∫ßn c·∫≠p nh·∫≠t tr·ªçng s·ªë s·ª≠ d·ª•ng 16 m·∫´u d·ªØ li·ªáu.
- `validation_data`: Ki·ªÉm tra tr√™n t·∫≠p test.

---

## 9. ƒê√°nh gi√° m√¥ h√¨nh

```python
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy}')
```

**Gi·∫£i th√≠ch:**

- `model.evaluate()`: ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p ki·ªÉm tra.
- `print(f'Test Accuracy: {accuracy}')`: Hi·ªÉn th·ªã ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p test.

---

## 10. V·∫Ω bi·ªÉu ƒë·ªì loss v√† accuracy

```python
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()
```

**Gi·∫£i th√≠ch:**

- `history.history['loss']`: Gi√° tr·ªã loss c·ªßa t·∫≠p hu·∫•n luy·ªán.
- `history.history['val_loss']`: Gi√° tr·ªã loss c·ªßa t·∫≠p ki·ªÉm tra.
- `plt.plot()`: V·∫Ω bi·ªÉu ƒë·ªì gi√∫p quan s√°t qu√° tr√¨nh h·ªçc c·ªßa m√¥ h√¨nh.

---
